from gen_ai_hub.orchestration.models.message import Message

from dataclasses import dataclass
from typing import List, Optional, Dict, Any

@dataclass
class ChatDelta:
    """
    Represents a partial update in a streaming chat response.

    Attributes:
        content: The text content of the chat delta.
        role: Optional role identifier (e.g., 'assistant', 'user') for the message delta.
    """
    content: str
    role: Optional[str] = None

@dataclass
class LLMUsage:
    """
    Represents the token usage statistics for an LLM (Large Language Model) operation.

    Attributes:
        completion_tokens: The number of tokens generated by the model in the response.
        prompt_tokens: The number of tokens in the input prompt.
        total_tokens: The total number of tokens used, including both prompt and completion tokens.
    """
    completion_tokens: int
    prompt_tokens: int
    total_tokens: int

@dataclass
class LLMChoice:
    """
    Represents an individual choice or response generated by the LLM.
    
    Attributes:
        index: The index of this particular choice in the list of possible choices.
        message: The message object containing the role and content of the response.
        finish_reason: The reason why the model stopped generating tokens.
        logprobs: Optional dictionary containing token log probabilities.
    """
    index: int
    message: Message
    finish_reason: str
    logprobs: Optional[Dict[str, float]] = None

@dataclass
class LLMChoiceStreaming:
    """
        Represents a streaming choice or partial response generated by the LLM.

        Attributes:
            index: The index of this particular choice in the list of possible choices.
            delta: The partial update (ChatDelta) for this choice.
            finish_reason: Optional reason for why the generation stopped, may be None during streaming.
            logprobs: Optional dictionary containing token log probabilities.
    """
    index: int
    delta: ChatDelta
    finish_reason: Optional[str] = None
    logprobs: Optional[Dict[str, float]] = None

@dataclass
class BaseLLMResult:
    """
    Base class for LLM results containing common attributes.

    Attributes:
        id: Unique identifier for the LLM operation.
        object: Type of object returned (e.g., "chat.completion").
        created: Timestamp when this result was created.
        model: Name or identifier of the model used.
    """
    id: str
    object: str
    created: int
    model: str

@dataclass
class LLMResult(BaseLLMResult):
    """
    Represents the complete result from an LLM operation.

    Attributes:
        id: The unique identifier for this LLM operation.
        object: The type of object returned (typically "chat.completion").
        created: The timestamp when this result was created.
        model: The name or identifier of the model used for generating the result.
        choices: A list of possible choices generated by the LLM.
        usage: The token usage statistics for this operation.
        system_fingerprint: An optional system fingerprint for tracking the model used.
    """
    choices: List[LLMChoice]
    usage: LLMUsage
    system_fingerprint: Optional[str] = None

@dataclass
class LLMResultStreaming(BaseLLMResult):
    """
        Represents a streaming result from an LLM operation.

        Attributes:
            id: The unique identifier for this LLM operation.
            object: The type of object returned (typically "chat.completion.chunk").
            created: The timestamp when this result was created.
            model: The name or identifier of the model used.
            choices: A list of streaming choices generated by the LLM.
            system_fingerprint: An optional system fingerprint for tracking the model used.
    """
    choices: List[LLMChoiceStreaming]
    system_fingerprint: Optional[str] = None

@dataclass
class GenericModuleResult:
    """
    Represents a generic module result in the orchestration process.

    Attributes:
        message: A message or description generated by the module.
        data: Additional data relevant to the module result.
    """
    message: str
    data: Optional[Dict[str, Any]] = None

@dataclass
class BaseModuleResults:
    """
        Base class for module results containing grounding, common filtering and masking attributes.

        Attributes:
            input_filtering: Results from the input filtering module.
            output_filtering: Results from the output filtering module.
            input_masking: Results from the input masking module.
            grounding: A list of extracted text to be provided as grounding context.
    """
    input_filtering: Optional[GenericModuleResult] = None
    output_filtering: Optional[GenericModuleResult] = None
    input_masking: Optional[GenericModuleResult] = None
    grounding: Optional[GenericModuleResult] = None

@dataclass
class ModuleResults(BaseModuleResults):
    """
    Represents the results of various modules used in processing an orchestration request.

    Attributes:
        templating: A list of messages that define the conversation's context or template.
        llm: The result from the LLM operation.
        input_filtering: The result of any input filtering, if applicable.
        output_filtering: The result of any output filtering, if applicable.
        input_masking: The result of input masking, if applicable.
        output_unmasking: The result of output unmasking, if applicable.
    """
    llm: Optional[LLMResult] = None
    templating: Optional[List[Message]] = None
    output_unmasking: Optional[List[LLMChoice]] = None

@dataclass
class ModuleResultsStreaming(BaseModuleResults):
    """
        Represents the streaming results of various modules used in processing an orchestration request.

        Attributes:
            llm: The streaming result from the LLM operation.
            templating: A list of chat deltas that define the conversation's context or template.
            input_filtering: The result of any input filtering, if applicable.
            output_filtering: The result of any output filtering, if applicable.
            input_masking: The result of input masking, if applicable.
            output_unmasking: The result of output unmasking for streaming responses.
    """
    llm: Optional[LLMResultStreaming] = None
    templating: Optional[List[ChatDelta]] = None
    output_unmasking: Optional[List[LLMChoiceStreaming]] = None

@dataclass
class OrchestrationResponse:
    """
    Represents the complete response from an orchestration process.

    Attributes:
        request_id: The unique identifier for the request being processed.
        module_results: The results from the various modules involved in processing the request.
        orchestration_result: The final result from the orchestration, typically mirroring the LLM result.
    """
    request_id: str
    module_results: ModuleResults
    orchestration_result: LLMResult

@dataclass
class OrchestrationResponseStreaming:
    """
        Represents the streaming response from an orchestration process.

        Attributes:
            request_id: The unique identifier for the request being processed.
            module_results: The streaming results from the various modules involved in processing the request.
            orchestration_result: The streaming result from the orchestration.
    """
    request_id: str
    module_results: ModuleResultsStreaming
    orchestration_result: LLMResultStreaming

